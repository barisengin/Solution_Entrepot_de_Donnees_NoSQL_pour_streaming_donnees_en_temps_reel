{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.regression import RandomForestRegressor, LinearRegression, GBTRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import random, math, csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream de kinesis\n",
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "kinesis_client = boto3.client('kinesis', region_name='us-east-1')\n",
    "stream_name = 'medical-data' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializer un session Spark\n",
    "spark = SparkSession.builder.appName(\"App\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour générer des dossiers patients\n",
    "def generate_patient_added_event():\n",
    "    event = {\n",
    "            \"age\": random.randint(18, 65),\n",
    "            \"sex\": random.choice(['female', 'male']),\n",
    "            \"bmi\": round(random.uniform(15, 54), 2),\n",
    "            \"children\": random.randint(0, 5),\n",
    "            \"smoker\": random.choice(['no', 'yes']),\n",
    "            \"region\": random.choice(['northwest', 'northeast', 'southwest', 'southeast']),\n",
    "    }\n",
    "    return event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to generated_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Données générées\n",
    "data = [generate_patient_added_event() for _ in range(100)]\n",
    "\n",
    "csv_file = 'generated_data.csv'\n",
    "\n",
    "with open(csv_file, 'w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=data[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"Data written to {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour mettre les dossiers patients sur le stream de Kinesis\n",
    "def put_record(data, partition_key):\n",
    "    try:\n",
    "        response = kinesis_client.put_record(\n",
    "            StreamName=stream_name,\n",
    "            Data=json.dumps(data),\n",
    "            PartitionKey=partition_key\n",
    "        )\n",
    "        logger.info(\"Put record in stream %s.\", stream_name)\n",
    "        print(f\"Data {data} published into stream {stream_name} successfully.\")\n",
    "    except ClientError:\n",
    "        logger.exception(\"Couldn't put record in stream %s.\", stream_name)\n",
    "        raise\n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if __name__ == \\'__main__\\':\\n    try:\\n        while True:  \\n            event_data = generate_product_added_event()\\n            partition_key = f\"PK{random.randint(1, 7)}\"\\n\\n            put_record(event_data, partition_key)\\n\\n            time.sleep(5)\\n    except KeyboardInterrupt:\\n        print(\"\\nScript stopped by manual intervention!\")'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Décommentez pour démarrer le flux\n",
    "\"\"\"if __name__ == '__main__':\n",
    "    try:\n",
    "        while True:  \n",
    "            event_data = generate_patient_added_event()\n",
    "            partition_key = f\"PK{random.randint(1, 7)}\"\n",
    "\n",
    "            put_record(event_data, partition_key)\n",
    "\n",
    "            time.sleep(5)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nScript stopped by manual intervention!\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV à Parquet conversion pour Spark\n",
    "dfp = pd.DataFrame(data)\n",
    "df = pd.read_csv('generated_data.csv')\n",
    "\n",
    "df.to_parquet('generated_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+--------+------+---------+\n",
      "|age|   sex|  bmi|children|smoker|   region|\n",
      "+---+------+-----+--------+------+---------+\n",
      "| 41|female|38.36|       3|    no|southeast|\n",
      "| 38|  male| 34.9|       2|    no|southwest|\n",
      "| 38|female|47.72|       2|   yes|northwest|\n",
      "| 23|female|17.82|       4|    no|southeast|\n",
      "| 20|female|35.53|       3|   yes|northeast|\n",
      "+---+------+-----+--------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, sex: string, bmi: double, children: bigint, smoker: string, region: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_one = './generated_data.parquet'\n",
    "gendata = spark.read.parquet(file_path_one)\n",
    "gendata.show(5)\n",
    "gendata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: age, Type: <class 'int'>\n",
      "Key: sex, Type: <class 'str'>\n",
      "Key: bmi, Type: <class 'float'>\n",
      "Key: children, Type: <class 'int'>\n",
      "Key: smoker, Type: <class 'str'>\n",
      "Key: region, Type: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#Le type de chaque attribue\n",
    "event_data = generate_patient_added_event()\n",
    "event_data\n",
    "for key, value in event_data.items():\n",
    "    print(f\"Key: {key}, Type: {type(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+--------+------+---------+-----------+\n",
      "|age|   sex|   bmi|children|smoker|   region|    charges|\n",
      "+---+------+------+--------+------+---------+-----------+\n",
      "| 19|female|  27.9|       0|   yes|southwest|  16884.924|\n",
      "| 18|  male| 33.77|       1|    no|southeast|  1725.5523|\n",
      "| 28|  male|  33.0|       3|    no|southeast|   4449.462|\n",
      "| 33|  male|22.705|       0|    no|northwest|21984.47061|\n",
      "| 32|  male| 28.88|       0|    no|northwest|  3866.8552|\n",
      "+---+------+------+--------+------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, sex: string, bmi: double, children: bigint, smoker: string, region: string, charges: double]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = './insurance.parquet'\n",
    "insurance_df = spark.read.parquet(file_path)\n",
    "insurance_df.show(5)\n",
    "insurance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les données en deux pour l'apprentissage automatique\n",
    "train_df, test_df = insurance_df.randomSplit([.8, .2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les opérations pour convertir les données catégoriques afin de les inclure dans l'apprentissage automatique\n",
    "categorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\n",
    "\n",
    "index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
    "\n",
    "ohe_output_cols = [x + \"OHE\" for x in categorical_cols]\n",
    "\n",
    "string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\n",
    "\n",
    "ohe_encoder = OneHotEncoder(inputCols=index_output_cols, outputCols=ohe_output_cols)\n",
    "\n",
    "numeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\" or dataType == \"bigint\" ) & (field != \"charges\"))]\n",
    "\n",
    "assembler_inputs = numeric_cols + ohe_output_cols\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------------+------------------+------------------+\n",
      "|            features|    charges|     prediction_lr|    prediction_rfr|    prediction_gbt|\n",
      "+--------------------+-----------+------------------+------------------+------------------+\n",
      "|[18.0,24.09,1.0,1...|  2201.0971| 308.0785935124295| 5563.733118618825| 7644.366389283847|\n",
      "|[18.0,27.28,3.0,1...| 18223.4512|26033.038976080512|19734.727382680612|18955.511355551087|\n",
      "|[18.0,29.165,0.0,...|7323.734819|2741.4005389676386| 4390.787045228075|3226.3080245374485|\n",
      "|[18.0,31.35,0.0,1...|  1622.1885|2385.8281368435964| 5392.017914279037|3237.0723283114107|\n",
      "|[18.0,35.625,0.0,...| 2211.13075| 5058.635876565417| 5925.249686865232| 4697.659051744988|\n",
      "|[18.0,38.17,0.0,1...|  1631.6683| 4832.197332264159|  5505.72593005443|4037.9878465258766|\n",
      "|[18.0,40.185,0.0,...| 2217.46915| 6694.331408987377|5305.8546573876765| 4558.879697741631|\n",
      "|(8,[0,1,4,7],[18....|  1702.4553|-248.3999833738526| 4190.558226857154| 2058.545187597875|\n",
      "|(8,[0,1,4,7],[18....| 1708.92575| 1421.372539306898| 4190.558226857154|1755.5319765729405|\n",
      "|[18.0,26.18,2.0,0...|  2304.0022|1354.6579905880862| 5938.920583172954| 5147.797769188717|\n",
      "|(8,[0,1,2,7],[18....| 17178.6824| 25918.61441010344| 19313.63041900628|16822.158398677777|\n",
      "|(8,[0,1,4,7],[18....|   1712.227|2273.2972957766688| 4350.502315673269|1985.9840096249932|\n",
      "|[18.0,30.03,1.0,0...|  1720.3537| 2209.222936208971| 5628.801903809695| 4873.997530765236|\n",
      "|(8,[0,1,7],[18.0,...| 33732.6867|26959.706039246128| 29849.03184323242| 35305.93059635775|\n",
      "|[18.0,35.2,1.0,0....|    1727.54|4063.7286166084305| 6032.725605992804| 4993.435076319954|\n",
      "|(8,[0,1,3,4],[19....|   1727.785|-1943.767933192894| 4280.132278034633|1479.7129449833492|\n",
      "|(8,[0,1,3,4],[19....|   1737.376| 531.2976750771759| 4136.226592322251| 2701.507639024242|\n",
      "|[19.0,39.615,1.0,...| 2730.10785| 6914.717194223829| 6983.099696505944| 6393.984362093721|\n",
      "|(8,[0,1,4,6],[19....| 1632.03625| 979.0004407464985| 4234.881308861867|1649.7939682545236|\n",
      "|[19.0,27.265,2.0,...|22493.65964|  2781.59407196328| 6364.138974406557| 6337.539364681232|\n",
      "+--------------------+-----------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Les opérations d'apprentissage automatique\n",
    "lr = LinearRegression(labelCol=\"charges\", featuresCol=\"features\", predictionCol=\"prediction_lr\")\n",
    "\n",
    "rfr = RandomForestRegressor(labelCol=\"charges\", featuresCol=\"features\", predictionCol=\"prediction_rfr\")\n",
    "\n",
    "gbt = GBTRegressor(labelCol=\"charges\", featuresCol=\"features\", predictionCol=\"prediction_gbt\")\n",
    "\n",
    "pipeline = Pipeline(stages = [string_indexer, ohe_encoder, vec_assembler, lr, rfr, gbt])\n",
    "\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "pred_df = pipeline_model.transform(test_df)\n",
    "\n",
    "pred_df.select(\"features\", \"charges\", \"prediction_lr\", \"prediction_rfr\", \"prediction_gbt\").show(20)\n",
    "type(pred_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For linear regression:\n",
      "\n",
      "charges = 252.24*age + 358.71*bmi + 526.45*children + 229.56*sex + -23527.79*smoker + -278.62*regionOHE1 + 506.88*regionOHE2 + 860.72*regionOHE3 + 10176.96\n"
     ]
    }
   ],
   "source": [
    "lr_model = pipeline_model.stages[-3]\n",
    "lcr = []\n",
    "\n",
    "for i in range(len(lr_model.coefficients)):\n",
    "    lcr.append(round(lr_model.coefficients[i], 2))\n",
    "lr_i = round(lr_model.intercept, 2)\n",
    "\n",
    "print(\"For linear regression:\\n\")\n",
    "print(f\"charges = {lcr[0]}*age + {lcr[1]}*bmi + {lcr[2]}*children + {lcr[3]}*sex + {lcr[4]}*smoker + {lcr[5]}*regionOHE1 + {lcr[6]}*regionOHE2 + {lcr[7]}*regionOHE3 + {lr_i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5660.558250981996\n",
      "4479.665155130369\n",
      "5077.150858402546\n"
     ]
    }
   ],
   "source": [
    "#Les résultats pour mean squared error\n",
    "mse_lr = mean_squared_error(np.array(pred_df.select(\"charges\").collect()), np.array(pred_df.select(\"prediction_lr\").collect()))\n",
    "mse_rfr = mean_squared_error(np.array(pred_df.select(\"charges\").collect()), np.array(pred_df.select(\"prediction_rfr\").collect()))\n",
    "mse_gbt = mean_squared_error(np.array(pred_df.select(\"charges\").collect()), np.array(pred_df.select(\"prediction_gbt\").collect()))\n",
    "print(math.sqrt(mse_lr))\n",
    "print(math.sqrt(mse_rfr))\n",
    "print(math.sqrt(mse_gbt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3960.005525917079\n",
      "2909.6562143155616\n",
      "2864.958810031765\n"
     ]
    }
   ],
   "source": [
    "#Les résultats pour mean absolute error\n",
    "mae_lr = mean_absolute_error(np.array(pred_df.select(\"charges\").collect()), np.array(pred_df.select(\"prediction_lr\").collect()))\n",
    "mae_rfr = mean_absolute_error(np.array(pred_df.select(\"charges\").collect()), np.array(pred_df.select(\"prediction_rfr\").collect()))\n",
    "mae_gbt = mean_absolute_error(np.array(pred_df.select(\"charges\").collect()), np.array(pred_df.select(\"prediction_gbt\").collect()))\n",
    "print(mae_lr)\n",
    "print(mae_rfr)\n",
    "print(mae_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3818657533655208\n",
      "0.4342880909660014\n",
      "0.3065796460605302\n"
     ]
    }
   ],
   "source": [
    "#Les résultats pour mean absolute percentage error\n",
    "mape_lr = mean_absolute_percentage_error(np.array(pred_df.select(\"charges\").collect()), np.array(pred_df.select(\"prediction_lr\").collect()))\n",
    "mape_rfr = mean_absolute_percentage_error(np.array(pred_df.select(\"charges\").collect()), np.array(pred_df.select(\"prediction_rfr\").collect()))\n",
    "mape_gbt = mean_absolute_percentage_error(np.array(pred_df.select(\"charges\").collect()), np.array(pred_df.select(\"prediction_gbt\").collect()))\n",
    "print(mape_lr)\n",
    "print(mape_rfr)\n",
    "print(mape_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.toPandas().to_csv('prediction_insurance.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
